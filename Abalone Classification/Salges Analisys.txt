In terms of data i separated it into training, testing and holdut. I used 80% of the data for training and it is on the file abalone. I used 12% for testing and shown in the abalone test file. The last 8% is left as holdout data. For feature selection I used all avalilable data except gender. The K nearest neighbors uses a heap queue in its clasification. It has a O(nd) time complexity where n is the ammount of samples and d is features. I used a value of K=25 for its neighbors as it seems to be the best for this data. Its performance is decent as it takes around 26s to go through the testing data that are 800 samples. It is also decently accurate with the errors being not that far apart from the actual number except in some cases but had accurate predictions most consistently of the 3.Discrete Naive bayes uses a probability distribution as it classification. It has a time complexity of O(n) as it only iterates through the data once. This was by far the fastest algorithm in the program at 0.23s. It also has an ok as most of the results are accurate. The perceptron learner was the slowest and also the least accurate of the 3. It took 67s as they usually have a large training time of its backpropagation learner method. The results tend to cluster too much. This can be because of the many features.